{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2daa4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Summary\n",
    "This document corresponds to Exercise 1 of [this file](https://github.com/PerformanceEstimation/Learning-Performance-Estimation/blob/main/Course.pdf).\n",
    "\n",
    "The goal of this exercise is to compute the value of the worst-case ratio $\\frac{\\|x_{k+1}-x_\\star\\|^2}{\\|x_k-x_\\star\\|^2}$ when $x_\\star=\\textrm{argmin}_x f(x)$ for some $f$ that is $L$-smooth and $\\mu$-strongly convex, and where $x_{k+1}=x_k-\\gamma_k \\nabla f(x_k)$ is obtained from a gradient step (with stepsize $\\gamma_k$) from $x_k$.\n",
    "\n",
    "If [PEPit](https://pypi.org/project/PEPit/) is not already installed, please execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pepit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f97bd9-4ff5-40e2-9361-ce60d48c8f41",
   "metadata": {},
   "source": [
    "Exercises 1.1 to 1.6 are presented in the document, along with their corrections, and do not involve any numerics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12301935",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 1.7\n",
    "Complete the code for computing the worst-case behavior of the ratio $\\frac{\\|x_{k+1}-x_\\star\\|^2}{\\|x_k-x_\\star\\|^2}$ (we use $k=0$ without loss of generality and for readability below).\n",
    "\n",
    "As seen in the exercise file, this is done by looking for the worst-case value of $\\|x_{1}-x_\\star\\|^2$ (this is often referred to as the \"performance measure\" in the performance estimation framework, and corresponds to the objective function of the problem of computing the worst-case ratio), when $\\|x_0-x_\\star\\|^2 =1$ (which is often referred to as an \"initial condition\", as it quantifies the quality of the \"initial\" iterate).\n",
    "\n",
    "To see how to specify such things within the PEPit framework, we refer to [the documentation](https://pepit.readthedocs.io/), which contains numerous examples. In particular, one can check [this example](https://pepit.readthedocs.io/en/0.1.0/_modules/PEPit/examples/composite_convex_minimization/proximal_gradient.html#wc_proximal_gradient), which is a bit more complex than what is asked in the exercise, but very related.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PEPit import PEP\n",
    "from PEPit.functions import SmoothStronglyConvexFunction\n",
    "\n",
    "def wc_gradient(L, mu, gamma, verbose=1):\n",
    "    # It is intended to compute the worst-case convergence of gradient descent in terms of the distance to \n",
    "    # an optimal solution: || x_{k+1} - x_\\star ||^2 / || x_k - x_\\star ||^2.\n",
    "    # Note that we use k = 0 in the code below for readability.\n",
    "    \n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a strongly convex smooth function and a closed convex proper function\n",
    "    f = problem.declare_function(SmoothStronglyConvexFunction, mu=mu, L=L)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_\\star\n",
    "    xs = f.stationary_point()\n",
    "\n",
    "    # Then define the point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()    \n",
    "    \n",
    "    # Perform one iteration of gradient descent\n",
    "    x = # TODO complete this line. Hint: use f.gradient() to call the gradient of f a a given point.\n",
    "\n",
    "    # Set the \"performance metric\" to the distance between x1 and xs\n",
    "    # TO COMPLETE (use \"problem.set_performance_metric\" to specify the objective function of the SDP)\n",
    "    problem.set_performance_metric( ) # TODO complete this line\n",
    "    \n",
    "    \n",
    "    # Set the \"initial condition\" to the distance between x0 and xs\n",
    "    # TO COMPLETE (use \"problem.set_initial_condition\" or \"problem.add_constraint\" to specify the\n",
    "    # constraint || x0 - xs ||^2 == 1).\n",
    "    problem.set_initial_condition( ) # TODO complete this line\n",
    "\n",
    "    # Solve the PEP\n",
    "    pepit_tau = problem.solve(verbose=verbose)\n",
    "    \n",
    "    # Return the worst-case convergence rate output by the SDP solver\n",
    "    return pepit_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4a1f8",
   "metadata": {},
   "source": [
    "Once the previous code is completed, one can test it for a few values of the problem and algorithmic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = .1\n",
    "L = 1\n",
    "gamma = 1\n",
    "verbose = 1\n",
    "\n",
    "pepit_tau = wc_gradient(L=L, mu=mu, gamma=gamma[i], verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9bc404-f223-4c42-bb65-de3b2439e6a3",
   "metadata": {},
   "source": [
    "### Exercise 1.8: optimal stepsize and range of acceptable stepsizes?\n",
    "\n",
    "Compute numerical values of the worst-case ratio for a few values of $\\mu$ and $\\gamma_k$ (with $L=1$), and try to infer rules for the stepsize choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389df0e5-68e8-427e-a80c-10c59084763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "nb_test = 20\n",
    "\n",
    "mu = .1\n",
    "L = 1\n",
    "gamma = np.linspace(0., 2., num=nb_test)\n",
    "verbose = 0\n",
    "\n",
    "pepit_taus = list()\n",
    "\n",
    "for i in range(nb_test):\n",
    "    t0= time.process_time()\n",
    "    pepit_tau = wc_gradient(L=L, mu=mu, gamma=gamma[i], verbose=verbose)\n",
    "    pepit_taus.append(pepit_tau)\n",
    "    t1 = time.process_time() - t0\n",
    "    print(i+1, '/', nb_test,' done (elapsed time:',\"%.2f\" %t1,'[s])')\n",
    "    \n",
    "plt.plot(gamma, pepit_taus, '-')\n",
    "\n",
    "plt.xlabel('Step size')\n",
    "plt.ylabel('||x_1-x_*||^2 / ||x_0-x_*||^2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6fc73a",
   "metadata": {},
   "source": [
    "### Exercise 1.9: variations (performance measures)\n",
    "Update the previous code for computing worst-case ratio $\\frac{\\|\\nabla f(x_{k+1})\\|^2}{\\|\\nabla f(x_k)\\|^2}$ and experiment with it.\n",
    "\n",
    "A good practice in PEPit is to limit the number of calls to evaluate function values and gradients. Indeed, each time a gradient or a function value is evaluated, it corresponds to (i) add points in the discrete representation of the worst-case function, and (ii) thereby, the problem also contain more \"interpolation inequalities\", rendering it numerically more complicated.\n",
    "\n",
    "\n",
    "Compute the worst-case ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e4589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86c8a15b-56e9-44b0-ad32-305d57db88ab",
   "metadata": {},
   "source": [
    "Perform numerical experiments for a few values of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8accc-c1ce-499b-8674-bbc42f3069bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b928df",
   "metadata": {},
   "source": [
    "### Exercise 1.10: variations (performance measures)\n",
    "Update the previous code for computing worst-case ratio $\\frac{f(x_{k+1})-f_\\star}{f(x_k)-f_\\star}$ and experiment with it.\n",
    "\n",
    "As before, limit as much as possible the number of gradient/function value evaluations. Note though that for certain classes of functions, PEPit detect when the gradient (or function value) at a point was already evaluated, and does not add the redundant points in the discrete representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62106ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b54e4a14",
   "metadata": {},
   "source": [
    "### Exercise 1.11: variations (number of iterations)\n",
    "Update the previous PEPit code for computing worst-case ratio $\\frac{\\|x_{N}-x_\\star\\|^2}{\\|x_0-x_\\star\\|^2}$ and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f73d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f14c570-0fb8-48ad-bca3-e495a601e639",
   "metadata": {},
   "source": [
    "### Exercise 1.12: dimension of the numerical worst-case\n",
    "\n",
    "This question does not require numerical experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc083582",
   "metadata": {},
   "source": [
    "### Exercise 1.13: identify low-dimensional counter examples\n",
    "The following code is an update of the previous one for computing the worst-case ratio $\\frac{\\|x_{N}-x_\\star\\|^2}{\\|x_0-x_\\star\\|^2}$. It could help identifying low dimensional worst-case examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6aa27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PEPit import PEP\n",
    "from PEPit.functions import SmoothStronglyConvexFunction\n",
    "\n",
    "def wc_gradient(L, mu, gamma, n, verbose=1):\n",
    "    # It is intended to compute a worst-case guarantee of gradient descent in terms of the distance to \n",
    "    # an optimal solution: ||x_{N} - x_\\star ||^2 / || x_0 - x_\\star \\\\^2.\n",
    "    \n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a strongly convex smooth function and a closed convex proper function\n",
    "    f = problem.declare_function(SmoothStronglyConvexFunction, mu=mu, L=L)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_\\star\n",
    "    xs = f.stationary_point()\n",
    "    fs = f(xs)\n",
    "\n",
    "    # Then define the point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()\n",
    "\n",
    "    # Gradient descent\n",
    "    x = x0\n",
    "    list_x = list() # store all x's\n",
    "    list_f = list() # store all f's\n",
    "    for i in range(n):\n",
    "        gx,fx = f.oracle(x)\n",
    "        list_x.append(x)\n",
    "        list_f.append(fx)\n",
    "        x = x - gamma * gx\n",
    "\n",
    "    # Set the \"performance metric\" to the distance between xN and xs\n",
    "    problem.set_performance_metric( (x-xs)**2 )\n",
    "    \n",
    "    # Set the \"initial condition\" to the distance between x0 and xs\n",
    "    problem.set_initial_condition( (x0-xs)**2 == 1) \n",
    "\n",
    "    # Solve the PEP with dimension_reduction_heuristic set to \"trace\" to use the trace heuristic\n",
    "    pepit_tau = problem.solve(verbose=verbose, dimension_reduction_heuristic=\"trace\")\n",
    "    \n",
    "    # INFO: for recovering points in the discrete representation of the function, you can use, e.g.:\n",
    "    # (x-xs).eval(), (x0-xs).eval()\n",
    "    # which correspond to the values of x_n-x_* and x_0-x_* obtained in the worst-case scenario.\n",
    "    \n",
    "    list_x_solved = list()\n",
    "    list_f_solved = list()\n",
    "    for i in range(n):\n",
    "        list_x_solved.append((list_x[i]-xs).eval())\n",
    "        list_f_solved.append((list_f[i]-fs).eval())\n",
    "        \n",
    "    # Return the output by the SDP solver\n",
    "    return pepit_tau, list_x_solved, list_f_solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1\n",
    "mu = .1\n",
    "gamma = 1/L \n",
    "n = 10\n",
    "\n",
    "# compute a low-dimensional worst-case example\n",
    "pepit_tau, list_x_solved, list_f_solved = wc_gradient(L, mu, gamma, n, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de91a8-f183-40dc-8777-cfc8fb085147",
   "metadata": {},
   "source": [
    "What is the dimension of the output? (hint: how many nonzero eigenvalue(s) does $G\\succcurlyeq 0$ contain? Check the output message of PEPit)\n",
    "\n",
    "In PEPit, if $G$ has $r$ nonzero eigenvalue(s), one can use the first $r$ coordinate of the output of the $x_k$'s and $g_k$'s for trying to represent a worst-case function, as follows.  What do you observe? (hint: try different values of the stepsize in $(0,2/L)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7efadf-54a7-48df-8742-76e4b877be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if there is only 1 nonzero eigenvalue, plot the 1-dimensional WC function:\n",
    "first_coordinate_x = list()\n",
    "for i in range(n):\n",
    "    first_coordinate_x.append(list_x_solved[i][0])\n",
    "    \n",
    "plt.plot(first_coordinate_x,list_f_solved,'.',label='iterates')\n",
    "plt.plot(first_coordinate_x[0],list_f_solved[0],'.',label='x0')\n",
    "plt.plot(0,0,'.',label='xs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe8fd0",
   "metadata": {},
   "source": [
    "### Exercise 1.14: Variations (no strong convexity)\n",
    "What is the value of the worst-case ratio $\\frac{\\|x_{N}-x_\\star\\|^2}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$? Can you deduce convergence of gradient descent from it? Can you extract/deduce simple counter-examples from the numerics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecdfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9e12fc2-738b-44d7-8f5f-22bcc4b14240",
   "metadata": {},
   "source": [
    "Update the previous code for computing worst-case ratios $\\frac{f(x_N)-f_\\star}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$; can you deduce the apparent dependence on $N$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74fa82-7af6-4eb9-b11b-101e94dddcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea1c3022-8963-4213-b356-6bca68e377bf",
   "metadata": {},
   "source": [
    "### Exercise 1.15: Variations (no strong convexity & alternate performance measure)\n",
    "Update the previous code for computing worst-case ratios $\\frac{\\|\\nabla f(x_N)\\|^2}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$; can you deduce the apparent dependence on $N$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a761d-24d5-4732-916b-538941f98430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d60135a-aba8-4ae2-b8a7-4028ed1a2dc6",
   "metadata": {},
   "source": [
    "### Exercise 1.16: Variations (no strong convexity & alternate performance measure)\n",
    "Update the previous code for computing worst-case ratios $\\frac{\\|x_N-x_\\star\\|^2}{\\|\\nabla f(x_0)\\|^2}$ when $\\mu=0$; how does it depend on $N$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9216088-b964-49e9-b108-4fcfd2318325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddf2f08a-396c-4499-8291-46c4f1952e78",
   "metadata": {},
   "source": [
    "### Exercise 1.17: learning outcomes\n",
    "\n",
    "This question does not require any numerical experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
