\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{bibentry}
\nobibliography*
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{fixltx2e}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

%\usepackage{url,textcomp}

\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{comment}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{pdfsync}
%\usepackage[svgnames]{xcolor}
\usepackage{pifont}
\hypersetup{
	colorlinks   = true, %Colours links instead of ugly boxes
	urlcolor     = blue, %Colour for external hyperlinks
	linkcolor    = blue, %Colour of internal links
	citecolor   = blue %Colour of citations
}
\usepackage{cleveref}

\renewcommand{\headrulewidth}{1pt}
\newcommand{\norm}[1]{{\left\lVert#1\right\rVert}}
\newcommand{\normsq}[1]{{\left\lVert#1\right\rVert}^2}
\newcommand{\Tr}[1]{{\trace\left(#1\right)}}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\inner}[2]{{\langle #1, #2\rangle}}
\DeclareMathOperator{\linspan}{span}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\rank}{rank}

\fancyhead[L]{}
\fancyhead[R]{}

\fancyfoot[C]{\textbf{page \thepage}}

\newcommand{\caution}[1]{{\color{red}{\sc Caution:} #1}}
\newcommand{\pesto}{{PESTO }}
\newcommand{\pepit}{{PEPit }}

\newtheorem{exercise}{Exercise}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\begin{document}
	\author{Adrien Taylor\footnote{INRIA, SIERRA project-team, and D.I. Ecole normale sup\'erieure, Paris, France. Email: adrien.taylor@inria.fr}, Baptiste Goujaud\footnote{CMAP, École Polytechnique, Institut Polytechnique de Paris, France. Email: baptiste.goujaud@gmail.com}}
	
	\title{Worst-case analyses for first-order optimization methods}
	\date{Current version: \today}
	\maketitle
	
	\renewcommand*\contentsname{}
	\setcounter{tocdepth}{2} \tableofcontents
	


	\section*{Foreword \& Acknowledgements}
	Those notes were written for accompanying the \href{https://trade-opt-itn.eu/workshop.html}{TraDE-OPT workshop on algorithmic and continuous optimization}. If you have any comment, remark, or if you found a typo/mistake, please don't hesitate to feedback the authors!
	
	\paragraph*{Funding.} A. Taylor acknowledges support from the European Research Council (grant SEQUOIA 724063). This work was partly funded by the French government under management of Agence Nationale de la Recherche as part of the ``Investissements d’avenir'' program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). The work of B. Goujaud is partially supported by ANR-19-CHIA-0002-01/chaire SCAI, and Hi!Paris. 
	
	\clearpage
	%==================================
	%								%||
	\section{Introduction}			%||
	%============================	%||
	%==================================
	
	This document provides a series of exercises for getting familiar with ``performance estimation problems'' and the use of semidefinite programming for analyzing the worst-case behaviors of first-order optimization methods. An informal introduction can be found in this \href{https://francisbach.com/computer-aided-analyses/}{blog post}.
	
	
In short, considering problems of the form $\min_x F(x)$ (we generally denote an optimal solution by $x_\star\in\argmin_x F(x)$), our goal is to access ``a priori'' the quality of the output (denoted by $x_k$) of some iterative algorithm. There are typically different ways of doing so, which might or might not be relevent depending on the target applications. In first-order optimization, we often want to upper bound the quality of $x_k$ in one of the following terms (which we all ideally would like to be as small as possible): $\|x_k-x_\star\|^2$, $\|\nabla f(x_k)\|^2$, or $f(x_k)-f(x_\star)$. There are of course other possibilities.

So, our goal is to assess the quality of $x_k$ by providing hopefully meaningfull upper bounds on (one of) those quantities. For doing so, we consider classes of problems (i.e., sets of assumptions on $F$), and perform worst-case analyses (i.e., we want the bound to be valid for all $F$ satisfying the set of assumptions at hand).
	
After studying the performance estimation framework for optimization methods, one can realize that it has a broader applicability for performing worst-case studies in numerical analysis (see exercises in \Cref{s:furtherex} and suggested readings in \Cref{s:readings} for further information). 

Notation and necessary background material is provided in \Cref{sec:background}.
	

	

	
	%==================================
	%								%||
	\section{Getting familial with performance estimation problems}			%||
	%============================	%||
	%==================================
	
	\begin{exercise}[Gradient method] For this exercise, consider the problem of ``black-box'' minimization of a smooth strongly convex function:
	\begin{equation}\label{eq:ex1_prob}
	f_\star \triangleq \min_{x\in\mathbb{R}^d} f(x),
	\end{equation}
	where $f$ is $L$-smooth and $\mu$-strongly convex (see \Cref{def:smoothstronglyconvex}), and where $x_\star\triangleq \argmin_{x} f(x)$ and $f_\star\triangleq f(x_\star)$ its optimal value. For minimizing~\eqref{eq:ex1_prob} we use gradient descent with a pre-determined sequence of step sizes $\{\gamma_k\}_k$; that is, we iterate $x_{k+1}=x_k-\gamma_k \nabla f(x_k)$.
	The goal of this exercise is to compute $\tau(\mu,L,\gamma_k)$, a.k.a.\! a convergence rate, the smallest value such that the inequality
	\[ \|x_{k+1}-x_\star\|^2 \leqslant \tau(\mu,L,\gamma_k) \|x_k-x_\star\|^2\]
	is valid for any $d\in\mathbb{N}$, for any $L$-smooth $\mu$-strongly convex function $f$ (notation $f\in\mathcal{F}_{\mu,L}$) and for all $x_k,x_{k+1}\in\mathbb{R}^d$ such that $x_{k+1}=x_k-\gamma_k \nabla f(x_k)$, and $x_\star=\argmin_x f(x)$. 
	\begin{enumerate}
	\item Show that 
	\begin{align*}
		\tau(\mu,L,\gamma_k)= \sup_{\substack{d,f\\x_k,x_{k+1},x_\star}} \ &\frac{\|x_{k+1}-x_\star\|^2}{\|x_k-x_\star\|^2}\\
		\text{s.t. } & f\in\mathcal{F}_{\mu,L} \\
		&x_{k+1}=x_k-\gamma_k  \nabla f(x_k)\\
		&\nabla f(x_\star)=0,
		\end{align*} where $f$, $x_k$, $x_{k+1}$, $x_\star$, and $d$ are the variables and $\mu$, $L$, $\gamma$ are parameters.
		
		Note that we will (sometimes abusively) use $\max$ instead of $\sup$ in the sequel as the optimum is usually attained for such problems (for this exercise, this is actually easy to show as the optimization problem is over a compact set).
	\item Show that
	\begin{equation*}
		\begin{aligned}
		\tau(\mu,L,\gamma_k)=\max_{\substack{x_k,x_{k+1},x_\star\\g_k,g_\star\\f_k,f_\star}} \quad & \frac{{\|x_{k+1}-x_\star\|}^2}{\|x_k-x_\star\|^2}\\
		\text{s.t. } & \exists f\in\mathcal{F}_{\mu,L} \text{ such that }\left\{\begin{array}{ll}
			f_i=f(x_i)\quad & i=k,\star\\
			g_i=f'(x_i)\quad & i=k,\star
			\end{array}\right.\\
		& x_{k+1}=x_k-\gamma_k  g_k\\
		& g_\star=0.\\
		\end{aligned}
		\end{equation*}
	\item Using \Cref{thm:interp_smoothstronglyconvex}, show that
	\begin{equation*}
		\begin{aligned}
		\tau(\mu,L,\gamma_k)=\max_{\substack{x_k,x_{k+1},x_\star\\g_k,g_\star\\f_k,f_\star}} \quad & \frac{{\|x_{k+1}-x_\star\|}^2}{\|x_k-x_\star\|^2}\\
		\text{s.t. } & f_\star\geqslant f_k+\inner{g_k}{x_\star-x_k}+\tfrac{1}{2L}\normsq{g_\star-g_k}+\tfrac{\mu}{2(1-\mu/L)}\normsq{x_\star-x_k-\tfrac{1}{L}(g_\star-g_k)}\\
			&f_k\geqslant f_\star+\inner{g_\star}{x_k-x_\star}+\tfrac{1}{2L}\normsq{g_k-g_\star}+\tfrac{\mu}{2(1-\mu/L)}\normsq{x_k-x_\star-\tfrac{1}{L}(g_k-g_\star)}\\
		& x_{k+1}=x_k-\gamma_k  g_k\\
		& g_\star=0.\\
		\end{aligned}
		\end{equation*}
	\item Show that
	\begin{equation*}
		\begin{aligned}
		\tau(\mu,L,\gamma_k)=\max_{\substack{x_k,x_{k+1},x_\star\\g_k,g_\star\\f_k,f_\star}} \quad &{\|x_{k+1}-x_\star\|}^2\\
		\text{s.t. } & f_\star\geqslant f_k+\inner{g_k}{x_\star-x_k}+\tfrac{1}{2L}\normsq{g_\star-g_k}+\tfrac{\mu}{2(1-\mu/L)}\normsq{x_\star-x_k-\tfrac{1}{L}(g_\star-g_k)}\\
			&f_k\geqslant f_\star+\inner{g_\star}{x_k-x_\star}+\tfrac{1}{2L}\normsq{g_k-g_\star}+\tfrac{\mu}{2(1-\mu/L)}\normsq{x_k-x_\star-\tfrac{1}{L}(g_k-g_\star)}\\
		& \|x_k-x_\star\|^2 = 1\\
		& x_{k+1}=x_k-\gamma_k  g_k\\
		& g_\star=0.\\
		\end{aligned}
		\end{equation*}
	\item Define $G$ and $F$
			\begin{align*}
			G \triangleq \begin{bmatrix}
			\|x_k-x_\star\|^2 & \langle g_k,x_k-x_\star\rangle\\
			\langle g_k, x_k-x_\star\rangle & \| g_k\|^2
			\end{bmatrix},\quad 	F \triangleq 			f_k-f_\star,
			\end{align*}
			(note that $G=[x_k-x_\star \quad g_k]^\top [x_k-x_\star \quad g_k]\succcurlyeq 0$). Show that $\tau(\mu,L,\gamma_k)$ can be computed using the following $2\times 2$ semidefinite program (SDP):  
			\begin{equation}\label{ex1:eq:SDP_grad}
			\begin{aligned}
			\tau(\mu,L,\gamma_k)=\max_{G,\, F} \quad & G_{1,1}+\gamma_k ^2 G_{2,2}-2\gamma_k G_{1,2}\\
			\text{s.t. } \quad & F + \tfrac{L\mu}{2(L-\mu)} G_{1,1}+\tfrac{1}{2(L-\mu)}G_{2,2}-\tfrac{L}{L-\mu}G_{1,2}\leqslant 0\\
			&-F + \tfrac{L\mu}{2(L-\mu)} G_{1,1}+\tfrac{1}{2(L-\mu)}G_{2,2}-\tfrac{\mu}{L-\mu}G_{1,2}\leqslant 0\\
			&G_{1,1}= 1\\
			&G\succcurlyeq 0,
			\end{aligned}
			\end{equation}
	\item Define $h_k\triangleq \gamma_k L$ and $\kappa=L/\mu$. Show that $\tau(\mu,L,\gamma_k)=\tau(1/\kappa,1,h_k)$ (in other words: we can study the case $L=1$ only and deduce the dependence of $\tau$ on $L$ afterwards).
	\item Complete the \href{https://github.com/PerformanceEstimation/Learning-Performance-Estimation/tree/main/Exercises - codes/Jupyter/Exercise1.ipynb}{\pepit code} (alternative in Matlab: \href{https://github.com/PerformanceEstimation/Learning-Performance-Estimation/blob/main/Exercises - codes/Matlab/Exercise1.m}{\pesto code}) for computing $\tau(\mu,L,\gamma_k)$ and compute its value for a few numerical values of $\mu$ and $\gamma_k$. 
	\item Using Lagrangian duality with the following primal-dual pairing ($\tau,\lambda_1,\lambda_2$ are dual variables):
	\begin{equation*}
			\begin{aligned}
			& F + \tfrac{L\mu}{2(L-\mu)} G_{1,1}+\tfrac{1}{2(L-\mu)}G_{2,2}-\tfrac{L}{L-\mu}G_{1,2}\leqslant 0&&:\lambda_1\\
			&-F + \tfrac{L\mu}{2(L-\mu)} G_{1,1}+\tfrac{1}{2(L-\mu)}G_{2,2}-\tfrac{\mu}{L-\mu}G_{1,2}\leqslant 0&&:\lambda_2\\
			&G_{1,1}= 1&&:\tau
			\end{aligned}
			\end{equation*}
	
	 one can show that
		\begin{equation}\label{eq:ex1_dual}	 
		\begin{aligned}
			\tau(\mu,L,\gamma_k)=\min_{\tau,\lambda_1,\lambda_2\geqslant 0} & \,\tau\\
			\text{s.t. }& S=\begin{bmatrix}
				\tau-1+\frac{\lambda_1 L\mu}{L-\mu } & \gamma_k-\frac{\lambda_1 (\mu +L)}{2 (L-\mu )} \\
				\gamma_k-\frac{\lambda_1 (\mu +L)}{2 (L-\mu )} & \frac{\lambda_1}{L-\mu }-\gamma_k^2 \\
			\end{bmatrix}\succcurlyeq 0\\
			&0=\lambda_1-\lambda_2.
		\end{aligned}
		\end{equation}
	Note that equility holds due to strong duality (for going further: obtain this dual formulation and prove strong duality using a Slater condition). 
	
	Show that any feasible point $(\tau,\lambda_1,\lambda_2)$ to~\eqref{eq:ex1_dual} corresponds to an upper bound on $\tau(\mu,L,\gamma_k)$ (i.e., $\tau(\mu,L,\gamma_k)\leqslant \tau$).
	
	\item Is there a simple closed-form expression for $\tau(\mu,L,\gamma_k)$? Hint \#1: can we solve~\eqref{eq:ex1_dual} in closed-form? Hint \#2: the objective is linear in $\tau$; the optimal solution (if it exists) is therefore necessarily on the boundary of the PSD cone; hence $\tau$ must be such that at least one eigenvalue of $S$ is zero.
	
	Does it match the numerical values obtained using the previous codes for computing $\tau(\mu,L,\gamma_k)$ numerically?
	\item How can we adapt the SDP formulation~\eqref{ex1:eq:SDP_grad} for computing the smallest possible $\tau$ such that the inequality
	\[ \|\nabla f(x_{k+1})\|^2 \leqslant \tau \|\nabla f(x_k)\|^2\]
	is valid for any $d\in\mathbb{N}$, for any $L$-smooth $\mu$-strongly convex function $f$ (notation $f\in\mathcal{F}_{\mu,L}$) and for all $x_k,x_{k+1}\in\mathbb{R}^d$ such that $x_{k+1}=x_k-\gamma_k \nabla f(x_k)$? Modify your previous code for computing such a bound. Can you guess a closed-form expression for it?
	
	For going further: a dual problem is given by
	\begin{equation}\label{eq:ex1_dual2}	 
		\begin{aligned}
			\min_{\tau,\lambda_1,\lambda_2\geqslant 0} & \,\tau\\
			\text{s.t. }& S=\begin{bmatrix}
		\tau+\lambda_1 \frac{(1-\gamma_k L)(1-\gamma_k \mu)}{L-\mu} & -\lambda_1\frac{2-\gamma_k(L+\mu)}{2(L-\mu)}\\
		-\lambda_1\frac{2-\gamma_k(L+\mu)}{2(L-\mu)} & \frac{\lambda_1}{L-\mu}-1
			\end{bmatrix}\succcurlyeq 0\\
			&0=\lambda_1-\lambda_2.
		\end{aligned}
		\end{equation} Is there a simple closed-form solution for this problem? 
	\item How can we adapt the SDP formulation~\eqref{ex1:eq:SDP_grad} for computing the smallest possible $\tau$ such that the inequality
	\[ f(x_{k+1})-f(x_\star) \leqslant \tau (f(x_{k})-f(x_\star))\]
	is valid for any $d\in\mathbb{N}$, for any $L$-smooth $\mu$-strongly convex function $f$ (notation $f\in\mathcal{F}_{\mu,L}$) and for all $x_k,x_{k+1}\in\mathbb{R}^d$ such that $x_{k+1}=x_k-\gamma_k \nabla f(x_k)$? Modify your previous code for computing such a bound. Can you guess a closed-form expression for it?
	
	For going further: using the following primal-dual pairing
	{\scriptsize\begin{equation*}
	\begin{aligned}
	& f(x_0)\geqslant f(x_\star)+\tfrac{1}{2L}\|\nabla f(x_0)\|^2+\tfrac{\mu}{2(1-\mu/L)}\|x_0-x_\star-\tfrac1L \nabla f(x_0)\|^2&&:\lambda_1\\
	& f(x_\star)\geqslant f(x_0)+\inner{\nabla f(x_0)}{x_\star-x_0}+\tfrac{1}{2L}\|\nabla f(x_0)\|^2+\tfrac{\mu}{2(1-\mu/L)}\|x_0-x_\star-\tfrac1L \nabla f(x_0)\|^2&&:\lambda_2\\
	& f(x_1)\geqslant f(x_\star)+\tfrac{1}{2L}\|\nabla f(x_1)\|^2+\tfrac{\mu}{2(1-\mu/L)}\|x_1-x_\star-\tfrac1L \nabla f(x_1)\|^2&&:\lambda_3\\
	& f(x_\star)\geqslant f(x_1)+\inner{\nabla f(x_1)}{x_\star-x_1}+\tfrac{1}{2L}\|\nabla f(x_1)\|^2+\tfrac{\mu}{2(1-\mu/L)}\|x_1-x_\star-\tfrac1L \nabla f(x_1)\|^2&&:\lambda_4\\
	& f(x_0)\geqslant f(x_1)+\inner{\nabla f(x_1)}{x_0-x_1}+\tfrac{1}{2L}\|\nabla f(x_0)-\nabla f(x_1)\|^2+\tfrac{\mu}{2(1-\mu/L)}\|x_1-x_0-\tfrac1L (\nabla f(x_1)-\nabla f(x_0))\|^2&&:\lambda_5\\
	& f(x_1)\geqslant f(x_0)+\inner{\nabla f(x_0)}{x_1-x_0}+\tfrac{1}{2L}\|\nabla f(x_0)-\nabla f(x_1)\|^2+\tfrac{\mu}{2(1-\mu/L)}\|x_1-x_0-\tfrac1L (\nabla f(x_1)-\nabla f(x_0))\|^2 &&:\lambda_6\\
	&f(x_0)-f(x_\star)= 1&&:\tau
	\end{aligned}
	\end{equation*}}
	a dual problem is given by
	{\scriptsize\begin{equation}\label{eq:ex1_dual3}	 
	\begin{aligned}
	\min_{\tau,\lambda_1,\lambda_2\geqslant 0} & \,\tau\\
	\text{s.t. }& \begin{bmatrix}
	\frac{\mu  L (\lambda_1+\lambda_2+\lambda_3+\lambda_4)}{L-\mu } & -\frac{L (\lambda_2+\gamma  \mu  (\lambda_3+\lambda_4))+\mu  \lambda_1}{L-\mu } & -\frac{L \lambda_4+\mu  \lambda_3}{L-\mu } \\
	* & \frac{\gamma  \mu  (\gamma  L (\lambda_3+\lambda_4+\lambda_5+\lambda_6)-2 \lambda_5)-2 \gamma  L \lambda_6+\lambda_1+\lambda_2+\lambda_5+\lambda_6}{L-\mu } & \frac{\gamma  L \lambda_4+\lambda_5 (\gamma  L-1)+\gamma  \mu  (\lambda_3+\lambda_6)-\lambda_6}{L-\mu } \\
	* & * & \frac{\lambda_3+\lambda_4+\lambda_5+\lambda_6}{L-\mu }
	\end{bmatrix}\succcurlyeq 0\\
	&0=\tau-\lambda_1+\lambda_2-\lambda_5+\lambda_6\\
	&1=-\lambda_3+\lambda_4+\lambda_5-\lambda_6,
	\end{aligned}
	\end{equation}}where ``$*$'' denotes symmetrical elements in the PSD matrix. Is there a simple closed-form solution for this problem? Hint \#1: plot some values for the multipliers; hint \#2: pick $\lambda_1=\lambda_3=\lambda_6=0$; does the problem simplify?
	\item Can we use this formalism for computing worst-case guarantees for a few iterations simultaneously? That is, to compute $\tau(\mu,L,\{\gamma_k\}_{k=0,\hdots,N-1})$ the smallest value such that the inequality
	\[ \|x_{N}-x_\star\|^2 \leqslant \tau(\mu,L,\{\gamma_k\}_{k=0,\ldots,N-1}) \|x_0-x_\star\|^2\]
	is valid for any $d\in\mathbb{N}$, for any $L$-smooth $\mu$-strongly convex function $f$ (notation $f\in\mathcal{F}_{\mu,L}$) and for all $x_0,x_1,\ldots,x_N\in\mathbb{R}^d$ such that $x_{k+1}=x_k-\gamma_k \nabla f(x_k)$ ($k=0,\ldots,N-1$), and $x_\star=\argmin_x f(x)$.
	\item What happens if $\mu=0$? Can you isolate the problem on a simple counter example? You can, for example, use this \href{https://github.com/PerformanceEstimation/Learning-Performance-Estimation/tree/main/Exercises - codes/Jupyter/Exercise1.ipynb}{\pepit code} (alternative in Matlab: \href{https://github.com/PerformanceEstimation/Learning-Performance-Estimation/blob/main/Exercises - codes/Matlab/Exercise1bis_dimreduction.m}{\pesto code}). Can you imagine a solution for avoiding such pathological behaviors in the analyses? What about studying guarantees of type\[ f(x_N)-f_\star \leqslant \tau(\mu,L,\{\gamma_k\}_{k=0,\ldots,N-1}) \|x_0-x_\star\|^2\]
	instead? Modify your code for studying such worst-case bounds, and try it numerically for the choice $\gamma_k=1/L$, $L=1$ and $\mu=0$. Guess the depence on $N$ based on a few numerical trials.
	\item Based on your current experience, what are, according to you, the key elements which allowed casting the worst-case analysis as an SDP?
	\item Can you write a standard proof for the linear convergence in terms of distance to an optimal point? in terms of convergence in gradient norm? and function values?
	\end{enumerate}
	\end{exercise}

	%==================================
	%								%||
	\section{Further exercises}\label{s:furtherex}		%||
	%============================	%||
	%==================================
	
	\begin{exercise}[Sublinear convergence of gradient descent and acceleration]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item show that it can be formulated as an SDP.
	\item numerical trials (code: XXX).
	\item Can we compute guarantees of type 
	\[ \min_{0\leqslant i\leqslant N} \|\nabla f(x_i)\|^2\leqslant \tau \|x_0-x_\star\|^2\]
	using semidefinite programming?
	\item Modify your code for computing the worst-case ratios $\frac{\min_{0\leqslant i\leqslant N}\|\nabla f(x_i)\|^2}{\|x_0-x_\star\|^2}$ and $\frac{\|\nabla f(x_N)\|^2}{\|x_0-x_\star\|^2}$ as functions of $N$. What can you conclude?
	\item Modify your code for computing worst-case guarantees for the following variant of Nesterov's accelerated gradient method:
	\begin{equation*}
	XXX
	\end{equation*}
	in terms of the same ratios, and compare them to those of gradient descent (as functions of $N$). What can you conclude?
	\end{enumerate}
	\end{exercise}
	
\begin{exercise}[Subgradient method]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item show that it can be formulated as ...
	\item show that the previous problem can be framed using a discrete version...
	\item numerical trials (code: XXX). Ex: modify the code to compute worst gradient norm, and worst best gradient norm among iterates
	\end{enumerate}
	\end{exercise}
	
	
	\begin{exercise}[Acceleration and Lyapunov analyses]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item show that it can be formulated as ...
	\item show that the previous problem can be framed using a discrete version...
	\item numerical trials
	\end{enumerate}
	\end{exercise}
	
	\begin{exercise}[Fixed-point iterations]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item two methods: Halpern and Kras...
	\item show that it can be formulated as ...
	\item show that the previous problem can be framed using a discrete version...
	\item show that it is equivalent to the SDP XXXX
	\item using duality show that ... (dual SDP)
	\item numerical trials
	\end{enumerate}
	\end{exercise}
	
	
	\begin{exercise}[Stochastic gradient descent]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item show that it can be formulated as ...
	\item show that the previous problem can be framed using a discrete version...
	\item show that it is equivalent to the SDP XXXX
	\item using duality show that ... (dual SDP)
	\item numerical trials
	\end{enumerate}
	\end{exercise}
	
	
	\begin{exercise}[Proximal point method]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item show that it can be formulated as ...
	\item show that the previous problem can be framed using a discrete version...
	\item show that it is equivalent to the SDP XXXX
	\item numerical trials
	\end{enumerate}
	\end{exercise}
	
	
	\begin{exercise}[Proximal gradient method]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item show that it can be formulated as ...
	\item show that the previous problem can be framed using a discrete version...
	\item show that it is equivalent to the SDP XXXX
	\item numerical trials
	\end{enumerate}
	\end{exercise}
	
	
	\begin{exercise}[Douglas-Rachford splitting]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item show that it can be formulated as ...
	\item show that the previous problem can be framed using a discrete version...
	\item show that it is equivalent to the SDP XXXX
	\item numerical trials
	\end{enumerate}
	\end{exercise}


	\begin{exercise}[Frank-Wolfe]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item show that it can be formulated as ...
	\item show that the previous problem can be framed using a discrete version...
	\item show that it is equivalent to the SDP XXXX
	\item numerical trials
	\end{enumerate}
	\end{exercise}	
	

	\begin{exercise}[Alternate projections \& Dykstra]
	Show that the smallest $\tau$ such that the inequality
	\[ ... \]
	\begin{enumerate}
	\item show that it can be formulated as ...
	\item show that the previous problem can be framed using a discrete version...
	\item show that it is equivalent to the SDP XXXX
	\item numerical trials
	\end{enumerate}
	\end{exercise}	
	
	
	%==================================
	%								%||
	\section{Background material and useful facts}			%||
	\label{sec:background}
	%============================	%||
	%==================================
	\subsection{Standard definitions}
	smoothness, strong convexity...
	\begin{definition}\label{def:ccp}
	cpp
	\end{definition}
	\begin{definition}\label{def:smoothstronglyconvex}
	sm str cvx (notation...)
	\end{definition}
	\begin{definition}\label{def:Lipschitzcvx}
	lip cvx
	\end{definition}
	\begin{definition}\label{def:Lipschitzcvx}
	lip cvx
	\end{definition}
	
	\subsection{Interpolation/extension theorems}
	This section gathers useful elements allowing to answer certain questions ...
	\begin{theorem}
	interpolation 1... (ccp)
	\end{theorem}
	\begin{theorem}\label{thm:interp_smoothstronglyconvex}
	interpolation 2... (smooth str convex)
	\end{theorem}
	\begin{theorem}
	interpolation 3... (smooth nonconvex)
	\end{theorem}
	\begin{theorem}
	nonsmooth
	\end{theorem}
	\begin{theorem}
	indicator
	\end{theorem}
	\subsection{Other useful inequalities}
	\subsection{SDP duality}
	++primal and dual SDP formulations
	\begin{theorem}
	slater
	\end{theorem}
	\section{Going further - suggested readings}\label{s:readings}
	\paragraph{Lyapunov analyses.}
	\paragraph{Designing methods.}
	\paragraph{Adaptive methods.}
	\paragraph{Primal-dual methods.} Ernest'
	\paragraph{Mirror descent.} Radu's
	\paragraph{Identifying lower complexity bounds.} QG, Radu's
	\paragraph{Continuous-time analyses.}
	\paragraph{Identifying counter-examples}
	\paragraph{Other analyses.}
	
	
\bibliographystyle{unsrt}
\bibliography{bib_}{}
\end{document}
