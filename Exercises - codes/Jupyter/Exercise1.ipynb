{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79c4e1e-f578-4429-acaf-323ecee09938",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This document corresponds to Exercise 1 of [this file](https://github.com/PerformanceEstimation/Learning-Performance-Estimation/blob/main/Exercises/Course.pdf).\n",
    "\n",
    "The first step consists in installing [PEPit](https://pypi.org/project/PEPit/) and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914c69b-65d4-4fad-9e2f-82245a750b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pepit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e901fe87-fb7d-403d-a7ad-95710cbcc57b",
   "metadata": {},
   "source": [
    "Secondly, complete the code for computing the worst-case behavior of the ratio $\\frac{\\|x_{k+1}-x_\\star\\|^2}{\\|x_k-x_\\star\\|^2}$ (we use $k=0$ without loss of generality and for readability below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de79a2-1459-4b9f-8cbe-465719ebac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PEPit import PEP\n",
    "from PEPit.functions import SmoothStronglyConvexFunction\n",
    "\n",
    "def wc_gradient(L, mu, gamma, verbose=1):\n",
    "    # It is intended to compute the worst-case convergence of gradient descent in terms of the distance to \n",
    "    # an optimal solution: ||x_{k+1} - x_\\star ||^2 / || x_k - x_\\star \\\\^2.\n",
    "    # Note that we use k = 0 in the code below for readability.\n",
    "    \n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a strongly convex smooth function and a closed convex proper function\n",
    "    f = problem.declare_function(SmoothStronglyConvexFunction, mu=mu, L=L)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_\\star\n",
    "    xs = f.stationary_point()\n",
    "\n",
    "    # Then define the point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()    \n",
    "    \n",
    "    # Perform one iteration of gradient descent\n",
    "    x1 = x0 - gamma * f.gradient(x0)\n",
    "\n",
    "    # Set the \"performance metric\" to the distance between x1 and xs\n",
    "    # TO COMPLETE (use \"problem.set_performance_metric\" to specify the objective function of the SDP)\n",
    "    problem.set_performance_metric( ) # complete this line\n",
    "    \n",
    "    # Set the \"initial condition\" to the distance between x0 and xs\n",
    "    # TO COMPLETE (use \"problem.set_initial_condition\" or \"problem.add_constraint\" to specify the\n",
    "    # constraint || x0 - xs ||^2 == 1).\n",
    "    problem.set_initial_condition( ) # complete this line\n",
    "\n",
    "    # Solve the PEP\n",
    "    pepit_tau = problem.solve(verbose=verbose)\n",
    "    \n",
    "    # Return the worst-case convergence rate output by the SDP solver\n",
    "    return pepit_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c47bf9-6501-41f7-bbfa-7ca0db24ef74",
   "metadata": {},
   "source": [
    "Once the previous code is completed, one can test it for a few values of the problem and algorithmic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148249f-251c-4597-b500-198e8dbf23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "nb_test = 20\n",
    "\n",
    "mu = .1\n",
    "L = 1\n",
    "gamma = np.linspace(0., 2., num=nb_test)\n",
    "verbose = 0\n",
    "\n",
    "pepit_taus = list()\n",
    "\n",
    "for i in range(nb_test):\n",
    "    t0= time.process_time()\n",
    "    pepit_tau = wc_gradient(L=L, mu=mu, gamma=gamma[i], verbose=verbose)\n",
    "    pepit_taus.append(pepit_tau)\n",
    "    t1 = time.process_time() - t0\n",
    "    print(i+1, '/', nb_test,' done [elapsed time:',\"%.2f\" %t1,'[s]')\n",
    "    \n",
    "plt.plot(gamma, pepit_taus, '-')\n",
    "\n",
    "plt.xlabel('Step size')\n",
    "plt.ylabel('||x_1-x_*||^2 / ||x_0-x_*||^2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb7217-41e1-4185-a62c-a16e6d320984",
   "metadata": {},
   "source": [
    "### Variations (performance measures)\n",
    "Update the previous code for computing worst-case ratios $\\frac{\\|\\nabla f(x_{k+1})\\|^2}{\\|\\nabla f(x_k)\\|^2}$ and $\\frac{f(x_{k+1})-f_\\star}{f(x_k)-f_\\star}$ and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef1ce8-a869-49b6-b7f9-2a3d893ace5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccea86ce-abd6-4508-a606-1994196245bd",
   "metadata": {},
   "source": [
    "### Variations (number of iterations)\n",
    "Update the previous code for computing worst-case ratio $\\frac{\\|x_{N}-x_\\star\\|^2}{\\|x_0-x_\\star\\|^2}$ and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a30f35-647c-47ab-a5e1-247dd77d2d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d77bfea2-4851-466b-97d4-dc3e83e6286e",
   "metadata": {},
   "source": [
    "## Identifying low-dimensional counter examples\n",
    "Update the previous code for computing worst-case ratios $\\frac{\\|x_{N}-x_\\star\\|^2}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$. What can you deduce convergence of gradient descent from it? Can you extract/deduce simple counter examples from the numerics?\n",
    "The following code could help picturing what a problem might be in such types of worst-case analyses, by trying to identify low dimensional worst-case examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91982b5b-b904-4584-8ca2-71e02616096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PEPit import PEP\n",
    "from PEPit.functions import SmoothStronglyConvexFunction\n",
    "\n",
    "def wc_gradient(L, mu, gamma, n, verbose=1):\n",
    "    # It is intended to compute a worst-case guarantee of gradient descent in terms of the distance to \n",
    "    # an optimal solution: ||x_{N} - x_\\star ||^2 / || x_0 - x_\\star \\\\^2.\n",
    "    \n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a strongly convex smooth function and a closed convex proper function\n",
    "    f = problem.declare_function(SmoothStronglyConvexFunction, mu=mu, L=L)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_\\star\n",
    "    xs = f.stationary_point()\n",
    "\n",
    "    # Then define the point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()\n",
    "\n",
    "    # Gradient descent\n",
    "    x = x0\n",
    "    for i in range(n):\n",
    "        g = f.gradient(x)\n",
    "        x = x - gamma * g\n",
    "\n",
    "    # Set the \"performance metric\" to the distance between xN and xs\n",
    "    problem.set_performance_metric( (x-xs)**2 ) \n",
    "    \n",
    "    # Set the \"initial condition\" to the distance between x0 and xs\n",
    "    problem.set_initial_condition( (x0-xs)**2 == 1) \n",
    "\n",
    "    # Solve the PEP\n",
    "    pepit_tau = problem.solve(verbose=verbose, dimension_reduction_heuristic=\"trace\")\n",
    "    \n",
    "    # Return the output by the SDP solver\n",
    "    return pepit_tau, (x-xs).eval(), (x0-xs).eval(), g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bb7fb-dc06-49b9-8010-b8c7cd3b4868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80f21679-7a10-409c-af2a-72cd485876a5",
   "metadata": {},
   "source": [
    "### Variations (no strong convexity)\n",
    "Update the previous code for computing worst-case ratios $\\frac{f(x_N)-f_\\star}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$; can you deduce the apparent dependence on $N$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89aca09-dede-4b7b-8bd6-49032de27b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
