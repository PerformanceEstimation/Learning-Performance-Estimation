{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2daa4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Summary\n",
    "This document corresponds to Exercise 1 of [this file](https://github.com/PerformanceEstimation/Learning-Performance-Estimation/blob/main/Exercises/Course.pdf).\n",
    "\n",
    "The first step consists in installing [PEPit](https://pypi.org/project/PEPit/) and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pepit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12301935",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 1.7\n",
    "Complete the code for computing the worst-case behavior of the ratio $\\frac{\\|x_{k+1}-x_\\star\\|^2}{\\|x_k-x_\\star\\|^2}$ (we use $k=0$ without loss of generality and for readability below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PEPit import PEP\n",
    "from PEPit.functions import SmoothStronglyConvexFunction\n",
    "\n",
    "def wc_gradient(L, mu, gamma, verbose=1):\n",
    "    # It is intended to compute the worst-case convergence of gradient descent in terms of the distance to \n",
    "    # an optimal solution: || x_{k+1} - x_\\star ||^2 / || x_k - x_\\star ||^2.\n",
    "    # Note that we use k = 0 in the code below for readability.\n",
    "    \n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a strongly convex smooth function and a closed convex proper function\n",
    "    f = problem.declare_function(SmoothStronglyConvexFunction, mu=mu, L=L)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_\\star\n",
    "    xs = f.stationary_point()\n",
    "\n",
    "    # Then define the point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()    \n",
    "    \n",
    "    # Perform one iteration of gradient descent\n",
    "    x = # TODO complete this line. Hint: use f.gradient() to call the gradient of f a a given point.\n",
    "\n",
    "    # Set the \"performance metric\" to the distance between x1 and xs\n",
    "    # TO COMPLETE (use \"problem.set_performance_metric\" to specify the objective function of the SDP)\n",
    "    problem.set_performance_metric( ) # TODO complete this line\n",
    "    \n",
    "    # Set the \"initial condition\" to the distance between x0 and xs\n",
    "    # TO COMPLETE (use \"problem.set_initial_condition\" or \"problem.add_constraint\" to specify the\n",
    "    # constraint || x0 - xs ||^2 == 1).\n",
    "    problem.set_initial_condition( ) # TODO complete this line\n",
    "\n",
    "    # Solve the PEP\n",
    "    pepit_tau = problem.solve(verbose=verbose)\n",
    "    \n",
    "    # Return the worst-case convergence rate output by the SDP solver\n",
    "    return pepit_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4a1f8",
   "metadata": {},
   "source": [
    "Once the previous code is completed, one can test it for a few values of the problem and algorithmic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "nb_test = 20\n",
    "\n",
    "mu = .1\n",
    "L = 1\n",
    "gamma = np.linspace(0., 2., num=nb_test)\n",
    "verbose = 0\n",
    "\n",
    "pepit_taus = list()\n",
    "\n",
    "for i in range(nb_test):\n",
    "    t0= time.process_time()\n",
    "    pepit_tau = wc_gradient(L=L, mu=mu, gamma=gamma[i], verbose=verbose)\n",
    "    pepit_taus.append(pepit_tau)\n",
    "    t1 = time.process_time() - t0\n",
    "    print(i+1, '/', nb_test,' done (elapsed time:',\"%.2f\" %t1,'[s])')\n",
    "    \n",
    "plt.plot(gamma, pepit_taus, '-')\n",
    "\n",
    "plt.xlabel('Step size')\n",
    "plt.ylabel('||x_1-x_*||^2 / ||x_0-x_*||^2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6fc73a",
   "metadata": {},
   "source": [
    "### Exercise 1.9: variations (performance measures)\n",
    "Update the previous code for computing worst-case ratio $\\frac{\\|\\nabla f(x_{k+1})\\|^2}{\\|\\nabla f(x_k)\\|^2}$ and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e4589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b928df",
   "metadata": {},
   "source": [
    "### Exercise 1.10: variations (performance measures)\n",
    "Update the previous code for computing worst-case ratio $\\frac{f(x_{k+1})-f_\\star}{f(x_k)-f_\\star}$ and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62106ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b54e4a14",
   "metadata": {},
   "source": [
    "### Exercise 1.11: variations (number of iterations)\n",
    "Update the previous PEPit code for computing worst-case ratio $\\frac{\\|x_{N}-x_\\star\\|^2}{\\|x_0-x_\\star\\|^2}$ and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f73d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc083582",
   "metadata": {},
   "source": [
    "### Exercise 1.13: identify low-dimensional counter examples\n",
    "Update the previous code for computing worst-case ratios $\\frac{\\|x_{N}-x_\\star\\|^2}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$. What can you deduce convergence of gradient descent from it? Can you extract/deduce simple counter examples from the numerics?\n",
    "The following code could help picturing what a problem might be in such types of worst-case analyses, by trying to identify low dimensional worst-case examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6aa27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PEPit import PEP\n",
    "from PEPit.functions import SmoothStronglyConvexFunction\n",
    "\n",
    "def wc_gradient(L, mu, gamma, n, verbose=1):\n",
    "    # It is intended to compute a worst-case guarantee of gradient descent in terms of the distance to \n",
    "    # an optimal solution: ||x_{N} - x_\\star ||^2 / || x_0 - x_\\star \\\\^2.\n",
    "    \n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a strongly convex smooth function and a closed convex proper function\n",
    "    f = problem.declare_function(SmoothStronglyConvexFunction, mu=mu, L=L)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_\\star\n",
    "    xs = f.stationary_point()\n",
    "\n",
    "    # Then define the point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()\n",
    "\n",
    "    # Gradient descent\n",
    "    x = x0\n",
    "    for i in range(n):\n",
    "        # TODO Complete the update\n",
    "\n",
    "    # Set the \"performance metric\" to the distance between xN and xs\n",
    "    problem.set_performance_metric( (x-xs)**2 )\n",
    "    \n",
    "    # Set the \"initial condition\" to the distance between x0 and xs\n",
    "    problem.set_initial_condition( (x0-xs)**2 == 1) \n",
    "\n",
    "    # Solve the PEP with dimension_reduction_heuristic set to \"trace\" to use the trace heuristic\n",
    "    pepit_tau = problem.solve(verbose=verbose, dimension_reduction_heuristic=\"trace\")\n",
    "    \n",
    "    # Return the output by the SDP solver\n",
    "    return pepit_tau, (x-xs).eval(), (x0-xs).eval(), g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fd133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4fe8fd0",
   "metadata": {},
   "source": [
    "### Exercise 1.14: Variations (no strong convexity)\n",
    "Update the previous code for computing worst-case ratios $\\frac{f(x_N)-f_\\star}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$; can you deduce the apparent dependence on $N$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecdfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea1c3022-8963-4213-b356-6bca68e377bf",
   "metadata": {},
   "source": [
    "### Exercise 1.15: Variations (no strong convexity & alternate performance measure)\n",
    "Update the previous code for computing worst-case ratios $\\frac{\\|\\nabla f(x_N)\\|^2}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$; can you deduce the apparent dependence on $N$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a761d-24d5-4732-916b-538941f98430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d60135a-aba8-4ae2-b8a7-4028ed1a2dc6",
   "metadata": {},
   "source": [
    "### Exercise 1.16: Variations (no strong convexity & alternate performance measure)\n",
    "Update the previous code for computing worst-case ratios $\\frac{\\|x_N-x_\\star\\|^2}{\\|\\nabla f(x_0)\\|^2}$ when $\\mu=0$; how does it depend on $N$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9216088-b964-49e9-b108-4fcfd2318325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
