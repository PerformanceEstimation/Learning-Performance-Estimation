{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This document corresponds to Exercise 1 of [this file](https://github.com/PerformanceEstimation/Learning-Performance-Estimation/blob/main/Exercises/Course.pdf).\n",
    "\n",
    "The first step consists in installing [PEPit](https://pypi.org/project/PEPit/) and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pepit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7\n",
    "Complete the code for computing the worst-case behavior of the ratio $\\frac{\\|x_{k+1}-x_\\star\\|^2}{\\|x_k-x_\\star\\|^2}$ (we use $k=0$ without loss of generality and for readability below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PEPit import PEP\n",
    "from PEPit.functions import SmoothStronglyConvexFunction\n",
    "\n",
    "def wc_gradient(L, mu, gamma, verbose=1):\n",
    "    # It is intended to compute the worst-case convergence of gradient descent in terms of the distance to \n",
    "    # an optimal solution: || x_{k+1} - x_\\star ||^2 / || x_k - x_\\star ||^2.\n",
    "    # Note that we use k = 0 in the code below for readability.\n",
    "    \n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a strongly convex smooth function and a closed convex proper function\n",
    "    f = problem.declare_function(SmoothStronglyConvexFunction, mu=mu, L=L)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_\\star\n",
    "    xs = f.stationary_point()\n",
    "\n",
    "    # Then define the point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()    \n",
    "    \n",
    "    # Perform one iteration of gradient descent\n",
    "    x = # TODO complete this line. Hint: use f.gradient() to call the gradient of f a a given point.\n",
    "\n",
    "    # Set the \"performance metric\" to the distance between x1 and xs\n",
    "    # TO COMPLETE (use \"problem.set_performance_metric\" to specify the objective function of the SDP)\n",
    "    problem.set_performance_metric( ) # TODO complete this line\n",
    "    \n",
    "    # Set the \"initial condition\" to the distance between x0 and xs\n",
    "    # TO COMPLETE (use \"problem.set_initial_condition\" or \"problem.add_constraint\" to specify the\n",
    "    # constraint || x0 - xs ||^2 == 1).\n",
    "    problem.set_initial_condition( ) # TODO complete this line\n",
    "\n",
    "    # Solve the PEP\n",
    "    pepit_tau = problem.solve(verbose=verbose)\n",
    "    \n",
    "    # Return the worst-case convergence rate output by the SDP solver\n",
    "    return pepit_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the previous code is completed, one can test it for a few values of the problem and algorithmic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "nb_test = 20\n",
    "\n",
    "mu = .1\n",
    "L = 1\n",
    "gamma = np.linspace(0., 2., num=nb_test)\n",
    "verbose = 0\n",
    "\n",
    "pepit_taus = list()\n",
    "\n",
    "for i in range(nb_test):\n",
    "    t0= time.process_time()\n",
    "    pepit_tau = wc_gradient(L=L, mu=mu, gamma=gamma[i], verbose=verbose)\n",
    "    pepit_taus.append(pepit_tau)\n",
    "    t1 = time.process_time() - t0\n",
    "    print(i+1, '/', nb_test,' done (elapsed time:',\"%.2f\" %t1,'[s])')\n",
    "    \n",
    "plt.plot(gamma, pepit_taus, '-')\n",
    "\n",
    "plt.xlabel('Step size')\n",
    "plt.ylabel('||x_1-x_*||^2 / ||x_0-x_*||^2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.10: variations (performance measures)\n",
    "Update the previous code for computing worst-case ratio $\\frac{\\|\\nabla f(x_{k+1})\\|^2}{\\|\\nabla f(x_k)\\|^2}$ and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.11: variations (performance measures)\n",
    "Update the previous code for computing worst-case ratio $\\frac{f(x_{k+1})-f_\\star}{f(x_k)-f_\\star}$ and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6: LMI (dual problem) for $\\frac{f(x_{k+1})-f_\\star}{f(x_k)-f_\\star}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import cvxpy as cp\n",
    "\n",
    "def lmi_functionvalues(L, mu, gamma):\n",
    "\n",
    "    # Write the LMI.\n",
    "    S = cp.Variable((3, 3))\n",
    "    lamb = cp.Variable(6)\n",
    "    tau = cp.Variable()\n",
    "\n",
    "    s11 = mu*L/(L-mu) * (lamb[0]+lamb[1]+lamb[2]+lamb[3])\n",
    "    s12 = -L/(L-mu) * (lamb[1]+gamma*mu*(lamb[2]+lamb[3])) - mu/(L-mu)*lamb[0]\n",
    "    s13 = -1/(L-mu) * (L*lamb[3]+mu*lamb[2])\n",
    "    s22 = 1/(L-mu) * (gamma*mu*(gamma*L*(lamb[2]+lamb[3]+lamb[4]+lamb[5])-2*lamb[4])-2*gamma*L*lamb[5]+lamb[0]+lamb[1]+lamb[4]+lamb[5])\n",
    "    s23 = 1/(L-mu) * (gamma*L*lamb[3]+lamb[4]*(gamma*L-1)+gamma*mu*(lamb[2]+lamb[5])-lamb[5])\n",
    "    s33 = 1/(L-mu) * (lamb[2]+lamb[3]+lamb[4]+lamb[5])\n",
    "    constraints = [0==tau-lamb[0]+lamb[1]-lamb[4]+lamb[5],\n",
    "                   1==-lamb[2]+lamb[3]+lamb[4]-lamb[5],\n",
    "                   S >> 0,\n",
    "                   S[0,0] == s11,\n",
    "                   S[1,1] == s22,\n",
    "                   S[2,2] == s33,\n",
    "                   S[0,1] == s12,\n",
    "                   S[1,0] == s12,\n",
    "                   S[0,2] == s13,\n",
    "                   S[2,0] == s13,\n",
    "                   S[1,2] == s23,\n",
    "                   S[2,1] == s23,\n",
    "                   lamb >= 0]\n",
    "    prob = cp.Problem(cp.Minimize(tau), constraints)\n",
    "    prob.solve()\n",
    "    return tau.value, lamb.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the numerical values of the multipliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "nb_test = 50\n",
    "\n",
    "mu = .1\n",
    "L = 1\n",
    "gamma = np.linspace(-1., 3., num=nb_test)\n",
    "verbose = 0\n",
    "taus = np.empty([nb_test])\n",
    "lambdas = np.empty([6, nb_test])\n",
    "\n",
    "for i in range(nb_test):\n",
    "    tau, lamb = lmi_functionvalues(L=L, mu=mu, gamma=gamma[i])\n",
    "    taus[i]=tau[()]\n",
    "    lambdas[:,i]=lamb\n",
    "\n",
    "\n",
    "plt.plot(gamma, lambdas[0,:], '-',label='$\\lambda_1$')\n",
    "plt.plot(gamma, lambdas[1,:], '-',label='$\\lambda_2$')\n",
    "plt.plot(gamma, lambdas[2,:], '-',label='$\\lambda_3$')\n",
    "plt.plot(gamma, lambdas[3,:], '-',label='$\\lambda_4$')\n",
    "plt.plot(gamma, lambdas[4,:], '-',label='$\\lambda_5$')\n",
    "plt.plot(gamma, lambdas[5,:], '-',label='$\\lambda_6$')\n",
    "plt.legend()\n",
    "plt.xlabel('$\\gamma$')\n",
    "plt.ylabel('Multipliers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.12: variations (number of iterations)\n",
    "Update the previous PEPit code for computing worst-case ratio $\\frac{\\|x_{N}-x_\\star\\|^2}{\\|x_0-x_\\star\\|^2}$ and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.13: identify low-dimensional counter examples\n",
    "Update the previous code for computing worst-case ratios $\\frac{\\|x_{N}-x_\\star\\|^2}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$. What can you deduce convergence of gradient descent from it? Can you extract/deduce simple counter examples from the numerics?\n",
    "The following code could help picturing what a problem might be in such types of worst-case analyses, by trying to identify low dimensional worst-case examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PEPit import PEP\n",
    "from PEPit.functions import SmoothStronglyConvexFunction\n",
    "\n",
    "def wc_gradient(L, mu, gamma, n, verbose=1):\n",
    "    # It is intended to compute a worst-case guarantee of gradient descent in terms of the distance to \n",
    "    # an optimal solution: ||x_{N} - x_\\star ||^2 / || x_0 - x_\\star \\\\^2.\n",
    "    \n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a strongly convex smooth function and a closed convex proper function\n",
    "    f = problem.declare_function(SmoothStronglyConvexFunction, mu=mu, L=L)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_\\star\n",
    "    xs = f.stationary_point()\n",
    "\n",
    "    # Then define the point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()\n",
    "\n",
    "    # Gradient descent\n",
    "    x = x0\n",
    "    for i in range(n):\n",
    "        # TODO Complete the update\n",
    "\n",
    "    # Set the \"performance metric\" to the distance between xN and xs\n",
    "    problem.set_performance_metric( (x-xs)**2 )\n",
    "    \n",
    "    # Set the \"initial condition\" to the distance between x0 and xs\n",
    "    problem.set_initial_condition( (x0-xs)**2 == 1) \n",
    "\n",
    "    # Solve the PEP with dimension_reduction_heuristic set to \"trace\" to use the trace heuristic\n",
    "    pepit_tau = problem.solve(verbose=verbose, dimension_reduction_heuristic=\"trace\")\n",
    "    \n",
    "    # Return the output by the SDP solver\n",
    "    return pepit_tau, (x-xs).eval(), (x0-xs).eval(), g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations (no strong convexity)\n",
    "Update the previous code for computing worst-case ratios $\\frac{f(x_N)-f_\\star}{\\|x_0-x_\\star\\|^2}$ when $\\mu=0$; can you deduce the apparent dependence on $N$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
